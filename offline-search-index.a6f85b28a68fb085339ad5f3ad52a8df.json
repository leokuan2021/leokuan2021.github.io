[{"body":" Code The repo for this project can be found here.  While I was studying for my CKA certificate, naturally I needed my own Kubernetes cluster to practice on and experiment with. Setting one up with VMs in the cloud was one option but I just so happened to have a NUC that I could repurpose as a testbed, which made the decision easy.\nI also needed to learn and practice Terraform and Ansible, so what better way to do all that than spinning up a 3-node Kubernetes cluster with some code? It goes without saying that this is not production grade at all but I believe it’s good enough for experimenting at home.\nThis project assumes Terraform and Ansible have been installed on the workstation from where the Terraform script and Ansible playbook will be run. It also assumes Proxmox has been installed on the target host and an API token has been created which can be used by the workstation to authenticate and communicate with Proxmox.\nTerraform is used at the beginning to provision the VMs, then Ansible takes over and continues with the system configuration and package installation for each of the nodes. It utilizes the kubeadm way to install Kubernetes. Calico, Helm, Kompose, Longhorn and MetalLB are also installed, among other things.\nThe end result is a 3-node Kubernetes VM cluster with the following parameters:\n Master Node: VMID = 140, hostname = k8s-master, IP = 192.168.1.140 Worker Node 1: VMID = 146, hostname = k8s-worker1, IP = 192.168.1.146 Worker Node 2: VMID = 147, hostname = k8s-worker2, IP = 192.168.1.147 Kubernetes control plane end point = 192.168.1.140:6443 Pod network CIDR = 192.168.152.0/23  Note that the user devops is created for each one of the 3 nodes and the default SSH key location and filename is used (~/.ssh/id_rsa).\n Proxmox and Cloud-init For hypervisor I went with Proxmox since I already had a little experience with it in the past. Installing Proxmox and building VM templates within it are both very straightforward. In order to create a template for VM instances that we can initialize using Terraform, we are going to utilize Cloud-init.\nCloud-init is a system for configuring OS on first boot. It is typically used on cloud-based systems but can also be used for non-cloud-based systems such as Proxmox or VirtualBox to which you can pass your cloud-init data such as instance metadata, network config and commands to be run as a CD-ROM and have the settings applied to your VMs.\nI ran the following commands on the Proxmox host to create the VM template. Official Proxmox documentation can be found here.\nCreating the VM Template # download the cloud image cd /var/lib/vz/template/iso/ wget https://cloud-images.ubuntu.com/focal/current/focal-server-cloudimg-amd64.img # install libguestfs-tools to directly install qemu-guest-agent into the iso apt-get install libguestfs-tools # install qemu-guest-agent virt-customize -a focal-server-cloudimg-amd64.img --install qemu-guest-agent # create a new VM qm create 1000 --name \"ubuntu-2004-cloudinit-template\" --memory 4096 --cores 2 --net0 virtio,bridge=vmbr0 # import the downloaded disk to local-lvm storage qm importdisk 1000 focal-server-cloudimg-amd64.img local-lvm # finally attach the new disk to the VM as scsi drive qm set 1000 --scsihw virtio-scsi-pci --scsi0 local-lvm:vm-1000-disk-0 # configure a CD-ROM drive, which will be used to pass the Cloud-Init data to the VM qm set 1000 --ide2 local-lvm:cloudinit # to be able to boot directly from the Cloud-Init image, set the bootdisk parameter to scsi0 qm set 1000 --boot c --bootdisk scsi0 # configure a serial console and use it as a display qm set 1000 --serial0 socket --vga serial0 # enable the agent qm set 1000 --agent 1 # convert the VM into a template qm template 1000  Create role and user, set privileges and generate API token pveum role add terraform-role -privs \"VM.Allocate VM.Clone VM.Config.CDROM VM.Config.CPU VM.Config.Cloudinit VM.Config.Disk VM.Config.HWType VM.Config.Memory VM.Config.Network VM.Config.Options VM.Monitor VM.Audit VM.PowerMgmt Datastore.AllocateSpace Datastore.Audit\" pveum user add terraform@pve pveum aclmod / -user terraform@pve -role terraform-role pveum user token add terraform@pve terraform-token --privsep=0 # output: root@host1:/var/lib/vz/template/iso# pveum user token add terraform@pve terraform-token --privsep=0 ┌──────────────┬──────────────────────────────────────┐ │ key │ value │ ╞══════════════╪══════════════════════════════════════╡ │ full-tokenid │ terraform@pve!terraform-token │ ├──────────────┼──────────────────────────────────────┤ │ info │ {\"privsep\":\"0\"} │ ├──────────────┼──────────────────────────────────────┤ │ value │ 87b3a427-5ebd-4451-b5c2-d19ef2008de4 │ └──────────────┴──────────────────────────────────────┘ Then on our workstation, we export the following to pass the API token when the TF script is run:\n# Export the following variables which our Terraform script will use to authenticate when it communicates with our Proxmox server export PM_API_TOKEN_ID=\"terraform@pve!terraform-token\" export PM_API_TOKEN_SECRET=\"87b3a427-5ebd-4451-b5c2-d19ef2008de4\"  Running the Terraform Script From the project root directory, do the following:\nterraform init terraform apply If all goes well, in a few minutes the cluster will be up and running, ready for you to practice your K8s chops on.\n","categories":"","description":"","excerpt":" Code The repo for this project can be found here.  While I was …","ref":"/iac/provision-k8s-cluster/","tags":["IaC","Kubernetes","Terraform","Ansible","Proxmox"],"title":"Provision a 3-Node K8S Cluster in Proxmox Using Terraform and Ansible"},{"body":" Code The repo for this project can be found here.  So I have this old USB only printer at home that I use every once in a blue moon. I used to have a Raspberry Pi hooked up to it that served as a print server and the same Pi had OctoPrint running on it for my 3d printer as well. But the Pi’s WiFi was on the verge of failing and would frequently drop its network connection so I figured I’d connect the printer to my Synology NAS and let it manage my laser printer instead. Sounds simple enough right?\nIt would’ve been awesome if it was plug-and-play but of course these things are never as straightforward as they should be. As it turns out, the Synology NAS only supports a select few printers out of the box and the only way that I can think of to get my printer to work with it is to pretty much duplicate the CUPS setup I had with the Raspberry Pi but with a docker container (and for x86 arch instead of ARM, of course).\nThe same thing has been done for a different printer, and an older version of the DSM. As of this writing, I am running DSM 7.0.1. So parts of that guide don’t really apply anymore.\nDockerfile Writing the dockerfile itself was very straightforward. I started with Debian Jessie as the base, and installed some basic utilities and build-essentials to build the driver from source. The P1006 is one of those annoying printers that require a firmware to be loaded on power up and that’s the reason installing the god damned driver is so much more complicated than it needs to be. Details on the installation steps for the foo2zjs driver can be found here. The steps can vary depending on the distro it’s to be installed on. Again, much more complicated than it needs to be. Finally Add the user, set a password, run the CUPS daemon, easy peasy.\ndocker build . -t leokuan/p1006cups docker push leokuan/p1006cups Synology DSM runs cupsd under the hood for its print service. To run the docker image, first check to see if cupsd is running, if so, use synosystemctl stop cupsd to terminate it (synoservice is deprecated). Don’t try to kill the process by ID as it will just respawn. You could also write a start-up script to automate the process. But I only needed to print a few pages at home so there wasn’t much of a reason to bother.\nsudo netstat -pna | grep 631 sudo synosystemctl stop cupsd sudo docker run -d -p 631:631 -p 5353:5353 --privileged -t -i --name p1006cups --device=/dev/bus/usb/001/001 leokuan/p1006cups:latest Of particular note, DSM now comes with lsusb, but it’s a strange Python implementation of lsusb that doesn’t really tell you the device numbers of your connected devices.\n$ lsusb |__usb1 1d6b:0002:0404 09 2.00 480MBit/s 0mA 1IF (Linux 4.4.180+ xhci-hcd xHCI Host Controller 0000:00:15.0) hub |__1-2 03f0:3e17:0100 00 2.00 480MBit/s 98mA 1IF (Hewlett-Packard HP LaserJet P1006 AC2DFF6) |__1-4 f400:f400:0100 00 2.00 480MBit/s 200mA 1IF (Synology DiskStation 7F00147B9D345A50) |__usb2 1d6b:0003:0404 09 3.00 5000MBit/s 0mA 1IF (Linux 4.4.180+ xhci-hcd xHCI Host Controller 0000:00:15.0) hub |__2-1 0bc2:ab34:0100 00 3.00 5000MBit/s 0mA 1IF (Seagate Backup+ Desk NA7H3L4F) Anyhow, I was able to pass the printer as device=/dev/bus/usb/001/001 to the container and that did the trick.\nThe rest is no different than setting up CUPS normally.\nIf you have more than a couple containers running on your NAS like I do, I recommend using portainer to manage them.\n","categories":"","description":"","excerpt":" Code The repo for this project can be found here.  So I have this old …","ref":"/miscellaneous/hp-p1006-docker/","tags":["Docker","Linux","CUPS"],"title":"Running a CUPS Print Server on Synology NAS for My Old USB HP P1006 Laser Printer Using Docker"},{"body":" Code The repo for this project can be found here.  Hawaii Bid Result Scraper Bid results for many public works construction projects are publicly available and I had this idea to mine the bid result data from the past 10 years.\nI wrote this scraper using Python with Scrapy for scraping public bid results to collect bid information such as bid amounts, listed subcontractors that can be used for analyzing business trends.\nFor example, the data collected can offer a rough approximation of how much work each of the major general contractors has at present so that estimators can have an idea how aggressive these companies are likely to be in terms of pursuing new work. Secondly, the data can also be analyzed to show how likely each of these general contractors is going to use certain subcontractors based on historic records.\n","categories":"","description":"","excerpt":" Code The repo for this project can be found here.  Hawaii Bid Result …","ref":"/miscellaneous/bid-result-scraper/","tags":["Python","Scrapy","HTML","CSS"],"title":"Construction Bid Result Scraper"},{"body":"This is a pretty neat Devops Technical Challenge that I think demonstrates my understanding of common Devops tools. I came across this tech challenge for a devops engineer job while lurking /r/devops looking for ideas for demo projects. The requirements are laid out in the gist below.  ","categories":"","description":"","excerpt":"This is a pretty neat Devops Technical Challenge that I think …","ref":"/devops/","tags":["Kubernetes","Docker","CI/CD","Gitlab","REST API","Node.js","MongoDB","Helm","Prometheus","Grafana","HPA"],"title":"DevOps Challenge"},{"body":" This is where I file IaC related things; hoping to add some AWS things here when I can find some free time.\n ","categories":"","description":"","excerpt":" This is where I file IaC related things; hoping to add some AWS …","ref":"/iac/","tags":["IaC"],"title":"Infrastructure as Code"},{"body":" Various random projects.\n ","categories":"","description":"","excerpt":" Various random projects.\n ","ref":"/miscellaneous/","tags":[""],"title":"Miscellaneous"},{"body":" Code: The repo for this project can be found here.  Creating and deploying k8s manifests for Rocket.chat We can make use of the official Docker image available for Rocket.chat. The application itself just needs a MongoDB database to get up and running and in the context of Kubernetes, that means creating a StatefulSet, for which we will also configure a volume claim to persist data for the application. Applying the rocketchat.yaml file will create a number of resources, namely the namespace, the service for the MongoDB, the statefulset again for the MongoDB, the deployment, as well as the service for the application itself.\nkubectl apply -f rocketchat.yaml From there, creating a MongoDB replica set is pretty straightforward — the StatefulSet resource allows for consistent identification of pods where each pod gets an ordinal name rather than a random name.\nWith the Service and the StatefulSet for the MongoDB in place, we can go ahead and configure the replicas. Since we set the number of replicas to 2 in our statefulset configuration, our MongoDB pods will be named rocketmongo-0 and rocketmongo-1. Note that the right way or the automatic way to configure the MongoDB replica set is by using a sidecar container, but since this is a quick and dirty demo I am opting to do it manually.\nStarting a bash shell inside the mongodb container:\nkubectl -n rocketchat exec rocketmongo-0 rocketmongo -it -- bash Once in, we check the FQDN for the container, which returns the following in our case\nroot@rocketmongo-0:/# hostname -f rocketmongo-0.rocketmongo.rocketchat.svc.cluster.local Similarly, the rocketmongo-1’s FQDN is rocketmongo-1.rocketmongo.rocketchat.svc.cluster.local\nLet’s go ahead and configure mongo:\nroot@rocketmongo-0:/# mongo \u003e rs.initiate({ _id: \"rs0\", version: 1, members: [ { _id: 0, host: \"rocketmongo-0.rocketmongo.rocketchat.svc.cluster.local:27017\" }, { _id: 1, host: \"rocketmongo-1.rocketmongo.rocketchat.svc.cluster.local:27017\" } ]}); { \"ok\" : 1 } To verify:\nrs0:SECONDARY\u003e rs.config() { \"_id\" : \"rs0\", \"version\" : 1, \"protocolVersion\" : NumberLong(1), \"writeConcernMajorityJournalDefault\" : true, \"members\" : [ { \"_id\" : 0, \"host\" : \"rocketmongo-0.rocketmongo.rocketchat.svc.cluster.local:27017\", \"arbiterOnly\" : false, \"buildIndexes\" : true, \"hidden\" : false, \"priority\" : 1, \"tags\" : { }, \"slaveDelay\" : NumberLong(0), \"votes\" : 1 }, { \"_id\" : 1, \"host\" : \"rocketmongo-1.rocketmongo.rocketchat.svc.cluster.local:27017\", \"arbiterOnly\" : false, \"buildIndexes\" : true, \"hidden\" : false, \"priority\" : 1, \"tags\" : { }, \"slaveDelay\" : NumberLong(0), \"votes\" : 1 } ], \"settings\" : { \"chainingAllowed\" : true, \"heartbeatIntervalMillis\" : 2000, \"heartbeatTimeoutSecs\" : 10, \"electionTimeoutMillis\" : 10000, \"catchUpTimeoutMillis\" : -1, \"catchUpTakeoverDelayMillis\" : 30000, \"getLastErrorModes\" : { }, \"getLastErrorDefaults\" : { \"w\" : 1, \"wtimeout\" : 0 }, \"replicaSetId\" : ObjectId(\"6281f013443a2f79c81e162b\") } } rs0:PRIMARY\u003e rs.isMaster() { \"hosts\" : [ \"rocketmongo-0.rocketmongo.rocketchat.svc.cluster.local:27017\", \"rocketmongo-1.rocketmongo.rocketchat.svc.cluster.local:27017\" ], \"setName\" : \"rs0\", \"setVersion\" : 1, \"ismaster\" : true, \"secondary\" : false, \"primary\" : \"rocketmongo-0.rocketmongo.rocketchat.svc.cluster.local:27017\", \"me\" : \"rocketmongo-0.rocketmongo.rocketchat.svc.cluster.local:27017\", \"electionId\" : ObjectId(\"7fffffff0000000000000001\"), \"lastWrite\" : { \"opTime\" : { \"ts\" : Timestamp(1652682800, 150), \"t\" : NumberLong(1) }, \"lastWriteDate\" : ISODate(\"2022-05-16T06:33:20Z\"), \"majorityOpTime\" : { \"ts\" : Timestamp(1652682800, 83), \"t\" : NumberLong(1) }, \"majorityWriteDate\" : ISODate(\"2022-05-16T06:33:20Z\") }, \"maxBsonObjectSize\" : 16777216, \"maxMessageSizeBytes\" : 48000000, \"maxWriteBatchSize\" : 100000, \"localTime\" : ISODate(\"2022-05-16T06:33:20.307Z\"), \"logicalSessionTimeoutMinutes\" : 30, \"minWireVersion\" : 0, \"maxWireVersion\" : 7, \"readOnly\" : false, \"ok\" : 1, \"operationTime\" : Timestamp(1652682800, 150), \"$clusterTime\" : { \"clusterTime\" : Timestamp(1652682800, 150), \"signature\" : { \"hash\" : BinData(0,\"AAAAAAAAAAAAAAAAAAAAAAAAAAA=\"), \"keyId\" : NumberLong(0) } } } rs0:PRIMARY\u003e rs.status() { \"set\" : \"rs0\", \"date\" : ISODate(\"2022-05-16T06:33:42.684Z\"), \"myState\" : 1, \"term\" : NumberLong(1), \"syncingTo\" : \"\", \"syncSourceHost\" : \"\", \"syncSourceId\" : -1, \"heartbeatIntervalMillis\" : NumberLong(2000), \"optimes\" : { \"lastCommittedOpTime\" : { \"ts\" : Timestamp(1652682818, 2), \"t\" : NumberLong(1) }, \"readConcernMajorityOpTime\" : { \"ts\" : Timestamp(1652682818, 2), \"t\" : NumberLong(1) }, \"appliedOpTime\" : { \"ts\" : Timestamp(1652682818, 2), \"t\" : NumberLong(1) }, \"durableOpTime\" : { \"ts\" : Timestamp(1652682818, 2), \"t\" : NumberLong(1) } }, \"electionCandidateMetrics\" : { \"lastElectionReason\" : \"electionTimeout\", \"lastElectionDate\" : ISODate(\"2022-05-16T06:33:01.852Z\"), \"electionTerm\" : NumberLong(1), \"lastCommittedOpTimeAtElection\" : { \"ts\" : Timestamp(0, 0), \"t\" : NumberLong(-1) }, \"lastSeenOpTimeAtElection\" : { \"ts\" : Timestamp(1652682771, 1), \"t\" : NumberLong(-1) }, \"numVotesNeeded\" : 2, \"priorityAtElection\" : 1, \"electionTimeoutMillis\" : NumberLong(10000), \"numCatchUpOps\" : NumberLong(0), \"newTermStartDate\" : ISODate(\"2022-05-16T06:33:01.894Z\"), \"wMajorityWriteAvailabilityDate\" : ISODate(\"2022-05-16T06:33:03.192Z\") }, \"members\" : [ { \"_id\" : 0, \"name\" : \"rocketmongo-0.rocketmongo.rocketchat.svc.cluster.local:27017\", \"health\" : 1, \"state\" : 1, \"stateStr\" : \"PRIMARY\", \"uptime\" : 266, \"optime\" : { \"ts\" : Timestamp(1652682818, 2), \"t\" : NumberLong(1) }, \"optimeDate\" : ISODate(\"2022-05-16T06:33:38Z\"), \"syncingTo\" : \"\", \"syncSourceHost\" : \"\", \"syncSourceId\" : -1, \"infoMessage\" : \"could not find member to sync from\", \"electionTime\" : Timestamp(1652682781, 1), \"electionDate\" : ISODate(\"2022-05-16T06:33:01Z\"), \"configVersion\" : 1, \"self\" : true, \"lastHeartbeatMessage\" : \"\" }, { \"_id\" : 1, \"name\" : \"rocketmongo-1.rocketmongo.rocketchat.svc.cluster.local:27017\", \"health\" : 1, \"state\" : 2, \"stateStr\" : \"SECONDARY\", \"uptime\" : 50, \"optime\" : { \"ts\" : Timestamp(1652682818, 2), \"t\" : NumberLong(1) }, \"optimeDurable\" : { \"ts\" : Timestamp(1652682818, 2), \"t\" : NumberLong(1) }, \"optimeDate\" : ISODate(\"2022-05-16T06:33:38Z\"), \"optimeDurableDate\" : ISODate(\"2022-05-16T06:33:38Z\"), \"lastHeartbeat\" : ISODate(\"2022-05-16T06:33:41.928Z\"), \"lastHeartbeatRecv\" : ISODate(\"2022-05-16T06:33:41.109Z\"), \"pingMs\" : NumberLong(0), \"lastHeartbeatMessage\" : \"\", \"syncingTo\" : \"rocketmongo-0.rocketmongo.rocketchat.svc.cluster.local:27017\", \"syncSourceHost\" : \"rocketmongo-0.rocketmongo.rocketchat.svc.cluster.local:27017\", \"syncSourceId\" : 0, \"infoMessage\" : \"\", \"configVersion\" : 1 } ], \"ok\" : 1, \"operationTime\" : Timestamp(1652682818, 2), \"$clusterTime\" : { \"clusterTime\" : Timestamp(1652682818, 2), \"signature\" : { \"hash\" : BinData(0,\"AAAAAAAAAAAAAAAAAAAAAAAAAAA=\"), \"keyId\" : NumberLong(0) } } } rs0:PRIMARY\u003e exit bye root@rocketmongo-0:/# exit exit There you go, now that we have MongoDB up and running, let’s go ahead and deploy the rocket chat server and its service. Note that we are running MetalLB for load balancer. pay attention in your environment variable, and make sure your URI connection is pointed to the replica set, otherwise you will not be able to connect if mongo0 is no longer the PRIMARY and get a MongoError: not master error, in situations such as after a reboot. https://www.mongodb.com/docs/manual/reference/connection-string/#examples\ndevops@k8s-master:~/part-a/k8s-manifests$ k -n rocketchat get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE rocketchat-server LoadBalancer 10.100.160.154 192.168.1.220 3000:32598/TCP 7m9s rocketmongo ClusterIP None \u003cnone\u003e 27017/TCP 7m9 we can go ahead and access our rocketchat server’s web interface by pointing our browser to the external ip assigned and you will be greeted by the admin setup page. Go ahead and fill out the account information.\nThat’s it. That’s one way to deploy Rocketchat.\ngo ahead and register a new account and the first account will become the admin account.\nCreating and deploying k8s manifests for GitLab To install gitlab using manifest is similar to docker. The main thing to configure here is persistent volumes and the ports for http, https and ssh.\nkubectl apply -f gitlab.yaml gitlab.yaml\nApplying the manifest will create the following:\nkubectl apply -f gitlab.yaml devops@k8s-master:~/part-a$ k -n gitlab get all NAME READY STATUS RESTARTS AGE pod/gitlab-848789fcd6-ktmct 1/1 Running 0 25m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/gitlab LoadBalancer 10.103.202.16 192.168.1.221 22:30001/TCP,80:30002/TCP,443:30003/TCP 24h NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/gitlab 1/1 1 1 25m NAME DESIRED CURRENT READY AGE replicaset.apps/gitlab-848789fcd6 1 1 1 25m Point your browser to the external-ip and you will be greeted by this page\nTroubleshooting Tips: Here are a few commands that may help troubleshoot if you run into problems.\n Here’s how one can watch the log as the container starts:  kubectl -n gitlab logs gitlab-848789fcd6-2h9td --follow  You can get a shell in the pod by doing the following:  kubectl -n gitlab exec -it gitlab-848789fcd6-ggm76 -- bash  Run gitlab-ctl status to show status, which can get you some general idea if anything such as puma keeps restarting:  gitlab-ctl status run: alertmanager: (pid 2095063) 89371s; run: log: (pid 1721296) 186791s run: gitaly: (pid 2095078) 89371s; run: log: (pid 1720630) 186892s run: gitlab-exporter: (pid 2095108) 89370s; run: log: (pid 1721103) 186809s run: gitlab-workhorse: (pid 2095110) 89369s; run: log: (pid 1721005) 186828s run: logrotate: (pid 2322981) 2968s; run: log: (pid 1720469) 186905s run: nginx: (pid 2095129) 89369s; run: log: (pid 2046027) 105649s run: node-exporter: (pid 2095213) 89368s; run: log: (pid 1721082) 186817s run: postgres-exporter: (pid 2095220) 89368s; run: log: (pid 1721317) 186787s run: postgresql: (pid 2095232) 89367s; run: log: (pid 1720718) 186885s run: prometheus: (pid 2095242) 89367s; run: log: (pid 1721158) 186799s run: puma: (pid 2330788) 21s; run: log: (pid 2046012) 105651s run: redis: (pid 2095285) 89366s; run: log: (pid 1720504) 186901s run: redis-exporter: (pid 2095291) 89366s; run: log: (pid 1721130) 186803s run: sidekiq: (pid 2095299) 89362s; run: log: (pid 1720957) 186835s   You can restart gitlab by running gitlab-ctl restart.\n  You can check a service’s log e.g. gitlab-ctl tail puma.\n  You can also check in /var/log/gitlab/gitlab-rails if gitlab-rails has some issues. Check application.log and production.log.`\n   While we are inside our gitlab instance’s bash shell, we may as well configure the following which we will need for the tasks to follow. Note that ordinarily these configs should be passed to the container as environment variables in the deployment manifest, which is the right way to do it, though the more right way is just use the official helm chart and edit its values to suit your use case. But since it’s my first time with Gitlab there’s some trial and error involved so I decided to configure it by editing /etc/gitlab/gitlab.rb directly as if it was a bare-metal installation:\nexternal_url is required external_url \"http://192.168.1.221\" while we are there, we will also want to enable the Gitlab Agent Server (KAS) which we will need later to deploy our app to our Kubernetes cluster using the pipeline we will build later ##! Enable GitLab KAS\ngitlab_kas['enable'] = true Reconfigure Omnibus GitLab with:\ngitlab-ctl reconfigure To log into your root account, one way to reset the password is by issuing the following command:\ndevops@k8s-master:~$ k -n gitlab exec -it gitlab-848789fcd6-2p885 -- gitlab-rake \"gitlab:password:reset[root]\" Enter password: Confirm password: Password successfully updated for user with username root. Refer to official documentation for other ways to set the password. By default, Omnibus GitLab automatically generates a password for the initial administrator user account (root) and stores it to /etc/gitlab/initial_root_password for at least 24 hours. For security reasons, after 24 hours, this file is automatically removed by the first gitlab-ctl reconfigure.\nNext thing you will want to do is set up a proper user account and start a new repo for the webhook-app that we will work on in the next step.\nWebhook App Code: The source for the webhook app can be found here.  The webhook app is very straightforward. I opted to use Node.js with Express to create the RESI API. The main thing is grab the token from our rocketchat’s incoming webhook and to keep things simple we will not have any authentication since there’s no requirement for it.\nConfiguring GitLab to build and deploy webhook-app Here comes the tricky part of this exercise. We have to set up the gitlab runner\nWe will install the helm chart way to install our gitlab runner and you are going to want to modify the default values for the helm chart. Once you have successfully deployed your runner, hit refresh on the settings/CICD page and you should see the runners shown as available. More official Gitlab documentation can be found here.\nNext, we can create our gitlab-ci.yml script (see source) and add our environment variables to gitlab In our case, we are adding our docker username and password and that will be it as far as configuring gitlab goes for our build stage.\nFor simplicity, we will skip the test stage for our simple app.\nAs for the deploy stage, we will need to configure the agent so that gitlab can deploy to our cluster.\nTo do that, follow official documentation, and here’s a video to walk you through the process as well.\nwe create a kubernetes-agent which contains information about the other projects that will be monitored by the agent, namely the webhook-app project. devops@k8s-master:~/part-a/k8s-manifests$ sudo docker run --pull=always --rm \\ \u003e registry.gitlab.com/gitlab-org/cluster-integration/gitlab-agent/cli:stable generate \\ \u003e --agent-token=oKsw2K2fzUD91KXJnDYa5j2wT3ay5Y-ZVpB9VDLZ82k3yynQFQ \\ \u003e --kas-address=wss://192.168.1.221/-/kubernetes-agent/ \\ \u003e --agent-version stable \\ \u003e --namespace gitlab-kubernetes-agent | kubectl apply -f - stable: Pulling from gitlab-org/cluster-integration/gitlab-agent/cli 2df365faf0e3: Pulling fs layer c6f4d1a13b69: Pulling fs layer 798d1822cd5f: Pulling fs layer 71a4deb2d4fa: Pulling fs layer 495aaf9276ed: Pulling fs layer 01fd94632652: Pulling fs layer 72ed240a3a17: Pulling fs layer 71a4deb2d4fa: Waiting 495aaf9276ed: Waiting 01fd94632652: Waiting 72ed240a3a17: Waiting 2df365faf0e3: Verifying Checksum 2df365faf0e3: Download complete 798d1822cd5f: Verifying Checksum 798d1822cd5f: Download complete 2df365faf0e3: Pull complete c6f4d1a13b69: Verifying Checksum c6f4d1a13b69: Download complete c6f4d1a13b69: Pull complete 798d1822cd5f: Pull complete 01fd94632652: Verifying Checksum 01fd94632652: Download complete 71a4deb2d4fa: Verifying Checksum 71a4deb2d4fa: Download complete 495aaf9276ed: Verifying Checksum 495aaf9276ed: Download complete 71a4deb2d4fa: Pull complete 72ed240a3a17: Verifying Checksum 72ed240a3a17: Download complete 495aaf9276ed: Pull complete 01fd94632652: Pull complete 72ed240a3a17: Pull complete Digest: sha256:edaeffc4fc5e5ab2c3b26c2f06775584e17e35f2720fb6d6319789bb613e8cbc Status: Downloaded newer image for registry.gitlab.com/gitlab-org/cluster-integration/gitlab-agent/cli:stable namespace/gitlab-kubernetes-agent created serviceaccount/gitlab-agent created clusterrolebinding.rbac.authorization.k8s.io/gitlab-agent-cluster-admin created secret/gitlab-agent-token-tb2m5m7tg8 created deployment.apps/gitlab-agent created Actually this will not work even though it says the agent is connected because we don’t have https. kubectl does not send authorization header if target is http.\nThe certificate way is the only way to get it to work without https. However kubectl only works over https so no luck with http unless you are willing to go through setting up more rolebinding and user account conifguration in kubernetes in a deprecated sort of way but I don’t see a point in continuing down this path.\nLet’s just enable letsencrypt and use https back to gitlab.rb and reconfigure it now.\nchange letsencrypt['enable'] = nil to letsencrypt['enable'] = true https://docs.gitlab.com/omnibus/settings/ssl.html#lets-encrypt-integration\nthen gitlab-ctl reconfigure\nTo expose our gitlab instance to the public internet and use our own domain name, I use a ngrok. Ngrok is something I use from time to time to experiment quick builds and I have a paid account so I can use it with my own domain.\nIt creates a secure tunnel and you don’t have to mess with router and firewall setting and it’s very simple kill once you’re done with your one-off thing. Of course one could run it directly on one of the nodes' localhost in the cluster and expose whatever service’s IP:nodeport from that endpoint, but there exists a neat ngrok-operator that makes running ngrok in kubernetes very simple.\nhelm repo add zufardhiyaulhaq https://charts.zufardhiyaulhaq.com/ helm install ngrok-operator zufardhiyaulhaq/ngrok-operator kubectl apply -f ngrok.yaml ngrok start --all --config=ngrok.yaml Then set up our kubernetes agent again similar to what we did before but with a proper https address this time. Now that we have both our runner and agent set up, we can then create our webhook-app.yaml file which contains the manifest for our webhook-app’s deployment and service. Then we can move on to the .gitlab-ci.yml to define our pipeline. For our build stage, We are going to use docker in docker to build our app into a container, tag it with commit short sha and push it to docker hub. For the deploy stage, we will replace the placeholder  with the latest commit_short_sha each time in our webhook-app.yaml file so that the correct image is pulled and deployed to our kubernetes cluster. Doing so ensures the correct image is deployed and allows us to easily rollback if we so desire, as opposed to always tagging the latest image latest, which really doesn’t make a whole of sense in this scenario.\nThis wraps up our pipeline creation. We can run our pipeline and see what happens.\n Set up incoming hook under integrations in rocketchat:\nfirst I set up a new bot account called gitlab. Then Under integrations, I created a new incoming webhook. After it’s saved, the webhook URL and token are generated for this webhook. To test it, we can use curl or postman or anything really. show a picture of test\ndevops@k8s-master:~$ curl -X POST -H 'Content-Type: application/json' --data '{\"text\":\"Example message\"}' http://rocketchat.leokuan.info/hooks/628f2bc88374570053f8945d/CG4d2wwWB2cd29W5ZXz7zfkWvWtJLE48YkkDwCBvoduoczgE Next, we set up the outgoing webhook in gitlab: putting it all together, let’s commit a change and watch this pipeline go.\nWhen we commit changes, we get a little notification on rocketchat Here’s our build stage: And here’s our deploy stage: devops@k8s-master:~$ k -n gitlab describe deployments.apps webhook-app Name: webhook-app Namespace: gitlab CreationTimestamp: Wed, 01 Jun 2022 08:04:20 +0000 Labels: app=webhook-app Annotations: deployment.kubernetes.io/revision: 2 Selector: app=webhook-app Replicas: 1 desired | 1 updated | 1 total | 1 available | 0 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 25% max unavailable, 25% max surge Pod Template: Labels: app=webhook-app Containers: webhook-app: Image: leokuan/webhook-app:87b16712 Port: \u003cnone\u003e Host Port: \u003cnone\u003e Environment: \u003cnone\u003e Mounts: \u003cnone\u003e Volumes: \u003cnone\u003e Conditions: Type Status Reason ---- ------ ------ Available True MinimumReplicasAvailable Progressing True NewReplicaSetAvailable OldReplicaSets: \u003cnone\u003e NewReplicaSet: webhook-app-6469bfcfcf (1/1 replicas created) Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ScalingReplicaSet 4m50s deployment-controller Scaled up replica set webhook-app-6469bfcfcf to 1 Normal ScalingReplicaSet 3m43s deployment-controller Scaled down replica set webhook-app-98cd4bb87 to 0 This concludes Part-A of the challenge. In Part-B we will work with Prometheus and Grafana to implement a simple monitoring solution, as well as configure auto scaling to take advantage of metrics.\n","categories":"","description":"","excerpt":" Code: The repo for this project can be found here.  Creating and …","ref":"/devops/sre-devops-challenge-part-a/","tags":["Kubernetes","Docker","CI/CD","Gitlab","REST API","Node.js","MongoDB","Helm","Prometheus","Grafana","HPA"],"title":"DevOps Challenge Part A"},{"body":"I made this Hugo site with Docsy theme to document some of the mini technical projects I worked on outside of work. I would love to be able to put everything and the kitchen sink in my resume and let prospective employers know that I am somewhat proficient when it comes to these technologies, but the general consensus seems to be that these details don’t really belong in a Technical Project Manager’s resume.\n","categories":"","description":"","excerpt":"I made this Hugo site with Docsy theme to document some of the mini …","ref":"/","tags":["intro"],"title":"Mostly Technical Things That Don't Fit in The Resume"},{"body":"In part A of this challenge we deployed a REST API server app which sends a direct message to Rocket.chat triggered by a Gitlab commit event. In part B of this challenge we build on top of this app to provide some monitoring capabilities by exposing and scraping Prometheus metrics.\nCode: The repo for this project can be found here.  Deploying Prometheus and Grafana to a Kubernetes Cluster    Prerequisite Deliverable Requirement     Helm values.yaml Deploy Prometheus and Grafana to your k8s cluster using the kube-prometheus-stack Helm chart    There are many ways to deploy Prometheus to a Kubernetes cluster but using the kube-prometheus-stack Helm chart as required here is arguably the most straightforward way to accomplish it. The community-maintained chart can be found here.\nFirst let’s add the repo:\nhelm repo add prometheus-community https://prometheus-community.github.io/helm-charts \u0026\u0026 helm repo update  Tip The kube-prometheus-stack helm chart’s values file is quite large at 2000+ lines, and if you’re working in the terminal in Vim like I was you might want to set foldmethod=indentthen use za, zo, and zc to interact with your folds and make it a bit easier to navigate, but firing up VScode is just as easy; whatever floats your boat.\nWhat I recommend for quick edits like this is save the default values.yaml for reference later if needed, make a copy, edit it, then do a vimdiff or colordiff to make sure the changes made are as intended before moving on to the next step.\n Save the chart’s default values to prometheus-grafana-values-default.yaml:\nhelm show values prometheus-community/kube-prometheus-stack \u003e prometheus-grafana-values-default.yaml Make a copy then we will go ahead and make some changes to prometheus-grafana-values.yaml:\ncp prometheus-grafana-values.yaml prometheus-grafana-values.yaml vim prometheus-grafana-values.yaml One could use ephemeral storage for testing out Prometheus and Grafana quickly but it’s probably not what you want for actual operations. Even though there’s no requirement for persistent storage in the instructions I decided to set it up anyway.\nFor this particular exercise we will focus on these three things:\n Persist Prometheus Persist Grafana Add ingress to Grafana  Configure persistent storage for Prometheus in our prometheus-grafana-values.yaml file:\n## Prometheus StorageSpec for persistent data ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/storage.md ## - storageSpec: {} + storageSpec:  ## Using PersistentVolumeClaim ## - # volumeClaimTemplate: - # spec: - # storageClassName: gluster - # accessModes: [\"ReadWriteOnce\"] - # resources: - # requests: - # storage: 50Gi + volumeClaimTemplate: + spec: + storageClassName: longhorn + accessModes: [\"ReadWriteOnce\"] + resources: + requests: + storage: 50Gi  # selector: {} ## Using tmpfs volume and configure persistent storage and ingress for Grafana:\ngrafana: enabled: true + ## Grafana's primary configuration + ## NOTE: values in map will be converted to ini format + ## ref: http://docs.grafana.org/installation/configuration/ + grafana.ini: + paths: + data: /var/lib/grafana/data + logs: /var/log/grafana + plugins: /var/lib/grafana/plugins + provisioning: /etc/grafana/provisioning + analytics: + check_for_updates: true + log: + mode: console + grafana_net: + url: https://grafana.net + server: + domain: grafana.leokuan.info + root_url: \"https://grafana.leokuan.info\" +  namespaceOverride: \"\" + ## Enable persistence using Persistent Volume Claims + ## ref: http://kubernetes.io/docs/user-guide/persistent-volumes/ + ## + persistence: + type: pvc + enabled: true + storageClassName: longhorn + accessModes: + - ReadWriteOnce + size: 10Gi + # annotations: {} + finalizers: + - kubernetes.io/pvc-protection + # subPath: \"\" + # existingClaim: @@ -668,25 +707,26 @@  ingress: ## If true, Grafana Ingress will be created - enabled: false + enabled: true  ## Annotations for Grafana Ingress - annotations: {} + annotations: + kubernetes.io/ingress.class: nginx  # kubernetes.io/tls-acme: \"true\" ## Labels to be added to the Ingress - labels: {} + labels: + app: grafana  ## Hostnames. ## Must be provided if Ingress is enable. ## - # hosts: - # - grafana.domain.com - hosts: [] - + hosts: + - grafana.leokuan.info +  ## Path for grafana ingress path: / After configuring the values file, we can finally install the helm chart.\nlet us first create a new namespace in our k8s cluster called monitoring for all our monitoring resources\nkubectl create namespace monitoring\nhelm install --namespace=monitoring prometheus prometheus-community/kube-prometheus-stack --values prometheus-grafana-values.yaml devops@k8s-master:~/part-b/prometheus-grafana-values$ helm install --namespace=monitoring prometheus prometheus-community/kube-prometheus-stack --values prometheus-grafana-values.yaml NAME: prometheus LAST DEPLOYED: Fri Jun 3 02:20:46 2022 NAMESPACE: monitoring STATUS: deployed REVISION: 1 NOTES: kube-prometheus-stack has been installed. Check its status by running: kubectl --namespace monitoring get pods -l \"release=prometheus\" The following resources will be created\ndevops@k8s-master:~/part-b/prometheus-grafana-values$ k -n monitoring get all NAME READY STATUS RESTARTS AGE pod/alertmanager-prometheus-kube-prometheus-alertmanager-0 2/2 Running 0 52s pod/prometheus-grafana-6d6498b446-lcdr9 3/3 Running 0 60s pod/prometheus-kube-prometheus-operator-7fc5d45d99-4c7g2 1/1 Running 0 60s pod/prometheus-kube-state-metrics-94f76f559-wjjdk 1/1 Running 0 60s pod/prometheus-prometheus-kube-prometheus-prometheus-0 2/2 Running 0 51s pod/prometheus-prometheus-node-exporter-48864 1/1 Running 0 60s pod/prometheus-prometheus-node-exporter-cklf9 1/1 Running 0 60s pod/prometheus-prometheus-node-exporter-p9r6r 1/1 Running 0 60s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/alertmanager-operated ClusterIP None \u003cnone\u003e 9093/TCP,9094/TCP,9094/UDP 52s service/prometheus-grafana ClusterIP 10.99.67.46 \u003cnone\u003e 80/TCP 61s service/prometheus-kube-prometheus-alertmanager ClusterIP 10.97.233.56 \u003cnone\u003e 9093/TCP 61s service/prometheus-kube-prometheus-operator ClusterIP 10.96.243.205 \u003cnone\u003e 443/TCP 61s service/prometheus-kube-prometheus-prometheus ClusterIP 10.110.118.134 \u003cnone\u003e 9090/TCP 61s service/prometheus-kube-state-metrics ClusterIP 10.99.11.14 \u003cnone\u003e 8080/TCP 61s service/prometheus-operated ClusterIP None \u003cnone\u003e 9090/TCP 51s service/prometheus-prometheus-node-exporter ClusterIP 10.99.26.63 \u003cnone\u003e 9100/TCP 61s NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.apps/prometheus-prometheus-node-exporter 3 3 3 3 3 \u003cnone\u003e 61s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/prometheus-grafana 1/1 1 1 61s deployment.apps/prometheus-kube-prometheus-operator 1/1 1 1 61s deployment.apps/prometheus-kube-state-metrics 1/1 1 1 61s NAME DESIRED CURRENT READY AGE replicaset.apps/prometheus-grafana-6d6498b446 1 1 1 60s replicaset.apps/prometheus-kube-prometheus-operator-7fc5d45d99 1 1 1 60s replicaset.apps/prometheus-kube-state-metrics-94f76f559 1 1 1 60s NAME READY AGE statefulset.apps/alertmanager-prometheus-kube-prometheus-alertmanager 1/1 52s statefulset.apps/prometheus-prometheus-kube-prometheus-prometheus 1/1 51s For the purpose of this exercise we will edit the service objects prometheus-grafana and prometheus-kube-prometheus-prometheus to use LoadBalancer instead of ClusterIP and set up DNS as well.\ndevops@k8s-master:~/part-b/prometheus-grafana-values$ k -n monitoring edit svc prometheus-grafana devops@k8s-master:~/part-b/prometheus-grafana-values$ k -n monitoring edit svc prometheus-kube-prometheus-prometheus selector: app.kubernetes.io/instance: prometheus app.kubernetes.io/name: grafana sessionAffinity: None type: LoadBalancer devops@k8s-master:~/part-b/prometheus-grafana-values$ k -n monitoring get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE alertmanager-operated ClusterIP None \u003cnone\u003e 9093/TCP,9094/TCP,9094/UDP 64m prometheus-grafana LoadBalancer 10.99.67.46 192.168.1.223 80:31733/TCP 64m prometheus-kube-prometheus-alertmanager ClusterIP 10.97.233.56 \u003cnone\u003e 9093/TCP 64m prometheus-kube-prometheus-operator ClusterIP 10.96.243.205 \u003cnone\u003e 443/TCP 64m prometheus-kube-prometheus-prometheus LoadBalancer 10.110.118.134 192.168.1.224 9090:30328/TCP 64m prometheus-kube-state-metrics ClusterIP 10.99.11.14 \u003cnone\u003e 8080/TCP 64m prometheus-operated ClusterIP None \u003cnone\u003e 9090/TCP 64m prometheus-prometheus-node-exporter ClusterIP 10.99.26.63 \u003cnone\u003e 9100/TCP 64m Optionally configure NodePort to expose your Prometheus and Grafana deployment to access their WebUI from your workstation’s browser if you don’t have a proper domain set up. Also note that if you don’t have valid certificates configured, Kubernetes will reject any client connections on HTTPS and thus Prometheus will show these kube system targets as being down: kube-controller-manager,kube-scheduler,kube-etcd. More on this here.\nprometheus-grafana and prometheus-kube-prometheus-prometheus are the service objects you may want to edit to configure that NodePort for the WebUI.\nNow go ahead and point your browser to the frontend of your Prometheus and Grafana instance and confirm both are working properly. The default admin user is admin and the password is prom-operator, which you can also retrieve by issuing the following commands:\nkubectl get secret --namespace monitoring prometheus-grafana -o jsonpath=\"{.data.admin-user}\" | base64 --decode ; echo kubectl get secret --namespace monitoring prometheus-grafana -o jsonpath=\"{.data.admin-password}\" | base64 --decode ; echo Adding a Prometheus metrics endpoint to webhook-app Requirement: Add a Prometheus metrics endpoint to your REST API that exposes a count of received webhooks.  For this we are going back to our nodejs webhook app. A quick search of npmjs returned two packages prom-client and express-prom-bundle which uses the former package under the hood. Prom-client looks well documented and simple enough to use for our simple case and seeing as I have no good reason to incur additional dependency I opted for prom-client. To install prom-client:\nnpm install prom-client Obviously the Counter type is what we will utilize to implement the webhook count metric, which only goes up and resets when the process restarts. We add the two mandatory parameters name and help and the counter is incremented when a POST request is made to the “/” route. Note that there’s no reset mechanism implemented for the counter, since there is no requirement for it. Let’s add the following code to the webhook-app.js file:\n/* ... */ const http = require('http') const server = http.createServer(app) const client = require('prom-client') let register = new client.Registry() const webhookCount = new client.Counter({ name: \"webhook_count\", help: \"Number of webhooks received\" }) /* ... */ register.registerMetric(webhookCount) register.setDefaultLabels({ app: 'webhook-app' }) /* ... */ app.post('/', (req, res) =\u003e { webhookCount.inc(1) /* ... */ }) Run it\nnode webhook-app.js Let’s test it out locally,\ncurl -X POST http://localhost:3000 curl http://localhost:3000/metrics # HELP webhook_count Number of webhooks received # TYPE webhook_count counter webhook_count{app=\"webhook-app\"} 2 One easy way to test is poll our metric endpoint using the watch command and have it refresh every 0.1s, note that it’s the minimum number. To generate webhook we can just send POST request to our API server and watch the number increase as\nxargs -I % -P 20 curl -X POST http://webhook-app.leokuan.info/ \u003c \u003c(printf '%s\\n' {1..50000}) Looks like prom-client works as advertised. Let’s build and test the image locally and push it to dockerhub.\ndocker build docker run Listening at xxxxxxxx docker push xxxxx Note: if maybe you’re wondering why I am not using the pipeline built in part A. There’s no requirement for using that in Part B’s instructions and I just felt like making the solution more stand-alone. It’s as simple as committing the webhook-app.js changes to our gitlab instance.\nMoving on, there’s no changes to our deployment.yaml and service.yaml on the Kubernetes side of things, since we pushed a new version of the app to Docker hub, let’s rollout the new version. Note that I am using :latest image for the deployment, best practice is to always pin everything to a specific version unless you have a good reason, quick and dirty is my reason and that’s good enough for MVS.\nkubectl -n gitlab rollout restart deployment webhook-app again, we can test the deployement again by curling the NodeIP:NodePort/metrics since we have the deployement exposed on nodeport. here’s the command and here’s the output\nConfiguring Prometheus to scrape using ServiceMonitor and creating a provisioned Grafana dashboard Requirement: Configure Prometheus to scrape that endpoint using ServiceMonitor, and create a provisioned Grafana dashboard for the metric.  The main thing about configuring ServiceMonitor is matching the app name and labels in your mainfests So what we really need to configure Prometheus is the following files:\nServiceMonitor Service\nThat’s it. and another you want to do is again expose nodeport so we can access it from our workstation outside the cluster.\nNext, we will create a provisioned Grafana dashboard, you can clicky click, but another way to do it is by code.\nAfter we created the dashboard using the UI and exported it to json which we will name webhook-app-dashboard.json, we can create the configmap for it.\ndevops@k8s-master:~/part-b/webhook-app-dashboard$ k -n monitoring create configmap grafana-webhook-app --from-file=webhook-app-dashboard.json --dry-run=client -o yaml \u003e webhook-app-dashboard-configmap.yaml We also need to add the grafana_dashboard label to it but kubectl doesn’t support the –labels=\"\" flag for create configmap, there’s no clean way to generate the configmap yaml imperatively with a one-liner so we will have to add it in the yaml manually.\nlabels: grafana_dashboard: \"1\" Alternatively, we can use another kubectl command to label the configmap\nkubectl -n monitoring label cm grafana-webhook-app grafana_dashboard=1 then run\nkubectl apply -f webhook-app-dashboard-configmap.yaml and you will be able to find the webhook-app dashboard in the UI.\nDeploying a custom metrics adaptor for Prometheus  Requirement: Create another custom metric for your REST API that exposes a count of in-flight requests. Deploy a custom metrics adaptor for Prometheus, saving your k8s manifest(s) and/or chart values.yaml.  let’s go back to the source code for the webhook-app and add the code for the in-flight counter. here’s the cdoe\nHere’s how I test it. Since there’s no requirement for what kind of traffic we need to generate, one simple way we can test the in-flight counter is by spamming HTTP requests using curl.\nxargs -I % -P 20 curl -X POST http://webhook-app.leokuan.info/ \u003c \u003c(printf '%s\\n' {1..50000}) You can increase the number of processes to further increase the queue length. This of course depends on your hardware and network spec so adjust as you need, we just need to push the server a little bit to make sure the code works as intended. Since we’re not actually doing anything with each request besides incrementing a counter, it’s very possible that no matter how many times requests you send the server can still keep up because we’re working with a gigabit local network there’s minimal latency. So what do we do? well we can simulate some using the tc command.\nLet’s again deploy it following the same step above and try that once it’s running on our cluster.\nAdding an HPA manifest to webhook-app  Requirement: Add an HPA manifest to your REST API that’s sensitive to the custom metric from step 1.  Finally, let’s add an HPA and make kubernetes handle the scaling automatically for us.\nLet’s try the traffic again and watch the deployment scale. After ramping up the traffic to trigger scaling in testing, there will be a cooldown period you have to wait a few minutes for it to scale down on its own. If you want to do it manually you have to manually edit the config map to patch to set the replicas to 1.\nFor whatever reason when I generate traffic on my mac it gets connection refused when I ramp up traffic, and one interesting thing happens when it works. It doesn’t direct traffic to the other newly brought up replicas, the queue length is still long for the origianl while it’s at zero for the other replicas. Generating with another machine running Ubuntu did not cause any issue, weird right?\nThis behavior raises two good points, one is our scaling strategy, we naively thought that taking the average was a good way, but that also exposes one problem, what if load balancing doesn’t work?\nOriginally I had thought this had something to do with the default kubernetes load balancing algorithm ,so I looked into and even tried vssd (no keepalive) to see what’s wrong, but didn’t find anything. It was good though because I learned about vssd, and you should look into it too if that’s something that you might find useful.\n","categories":"","description":"","excerpt":"In part A of this challenge we deployed a REST API server app which …","ref":"/devops/sre-devops-challenge-part-b/","tags":["Kubernetes","Docker","CI/CD","Gitlab","REST API","Node.js","MongoDB","Helm","Prometheus","Grafana","HPA"],"title":"DevOps Challenge Part B"},{"body":"Here’s a collection of repos of production IaC:\nOfficial Arch Linux Infrastructure Repository\nInfrastructure as code for GOV.UK GOV.UK AWS\nGitlab\nDoD IaC\nCanadian Digital Service\nhttps://github.com/cogini/multi-env-deploy\nhttps://github.com/teradici/cloud_deployment_scripts\nhttps://github.com/divviup/prio-server\nhttps://github.com/mozilla/partinfra-terraform\nhttps://github.com/cloudposse/\n","categories":"","description":"","excerpt":"Here’s a collection of repos of production IaC:\nOfficial Arch Linux …","ref":"/iac/production-grade-iac/","tags":["IaC","Learning"],"title":"Production Grade IaC Examples"},{"body":" Code The repo for this project can be found here.    I built this reaction trainer using an Arduino nano, a cheapo ebay monochrome OLED display and some bigass buttons. My wife saw professional badminton players train with it in Thailand and wanted one for her badminton club so I built one for her. The enclosure and the button housings were designed by me and 3d-printed with a Makergear printer, some parts were PETG, others TPU for obvious reasons.\n","categories":"","description":"","excerpt":" Code The repo for this project can be found here.    I built this …","ref":"/miscellaneous/onigoe-clone/","tags":["C/C++","Microcontroller","Circuits","PCB Layout","Product Design","Rapid Prototyping"],"title":"Onigoe Clone - A Reaction Trainer for Sports Using a Microcontroller"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/cups/","tags":"","title":"CUPS"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/docker/","tags":"","title":"Docker"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/linux/","tags":"","title":"Linux"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/","tags":"","title":"Tags"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/iac/","tags":"","title":"IaC"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/learning/","tags":"","title":"Learning"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/ansible/","tags":"","title":"Ansible"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/circuits/","tags":"","title":"Circuits"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/kubernetes/","tags":"","title":"Kubernetes"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/microcontroller/","tags":"","title":"Microcontroller"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/pcb-layout/","tags":"","title":"PCB Layout"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/product-design/","tags":"","title":"Product Design"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/proxmox/","tags":"","title":"Proxmox"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/rapid-prototyping/","tags":"","title":"Rapid Prototyping"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/terraform/","tags":"","title":"Terraform"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/css/","tags":"","title":"CSS"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/html/","tags":"","title":"HTML"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/python/","tags":"","title":"Python"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/scrapy/","tags":"","title":"Scrapy"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/gitlab/","tags":"","title":"Gitlab"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/grafana/","tags":"","title":"Grafana"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/helm/","tags":"","title":"Helm"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/hpa/","tags":"","title":"HPA"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/mongodb/","tags":"","title":"MongoDB"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/node.js/","tags":"","title":"Node.js"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/prometheus/","tags":"","title":"Prometheus"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/rest-api/","tags":"","title":"REST API"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/c/c++/","tags":"","title":"C/C++"},{"body":"","categories":"","description":"","excerpt":"","ref":"/categories/","tags":"","title":"Categories"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/ci/cd/","tags":"","title":"CI/CD"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/intro/","tags":"","title":"intro"},{"body":"","categories":"","description":"","excerpt":"","ref":"/search/","tags":"","title":"Search Results"}]